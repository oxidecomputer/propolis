= PhD TODOs

Here are some relatively shovel-ready TODOs as of 21 July 2022:

* Support for `cargo test` command line arguments
* Run servers in zones, or at least clean up their VM artifacts
* Support running multiple servers in one test case
* Add an affordance for testing multiple guest OS artifacts in a single run
* Add an affordance for selecting a non-default guest OS artifact in a test
* Support parameterized test cases
* Support for skipped tests

The sections below describe this work (very roughly--your humble author
apologizes for any word salad that appears below).

== Support for `cargo test`-style command line arguments

The runner doesn't support any kind of filtering or conditional test execution
today. It should take command-line parameters that permit this.

== Run servers in zones

"Bare" Propolis servers don't clean up their bhyve VMs on instance halt. (This
is tracked in the Propolis repo as
link:https://github.com/oxidecomputer/propolis/issues/154[issue #154]). In
production, Nexus will launch Propolis in an illumos zone and tear the zone down
when the instance is destroyed, freeing these resources. The test runner should
also do this.

This also allows future work to limit the CPUs/commit charge/etc. of Propolis
instances to flush out bugs.

An alternative, lower-tech approach is to scan the contents of `/dev/vmm` in a
fixture and use `bhyvectl` to clean up VMs after each test.

== Support running multiple serves in one test case

Today the VM factory can only produce a single server per test, which serves at
http://127.0.0.1:9000/. Running multiple Propolis servers is necessary to test
live migration.

== Affordances for multiple guest OS artifacts in a single run

Currently, the default guest OS is set on the runner's command line and all
tests are run once with that default. Because this is the only convenient way to
attach a boot disk with a guest OS image from the artifact store (see below),
this effectively limits a single invocation of the runner to a single set of
tests with a single guest OS.

A small but meaningful improvement would be to allow the runner to accept a
collection of guest OS artifact keys and to run all tests in the test set with
each of those artifact keys. That way you can test against Alpine, Debian,
Ubuntu, etc. in one go with out invoking the runner multiple times.

== Affordances for selecting non-default guest OS artifacts in a test

The VM factory has a `default_vm_config` routine that returns a VM with a single
attached virtio block disk whose image is selected from the artifact store.
There is no way to choose which image this is--it's always the default that was
stored when the factory was created.

Ideally, tests would be able to choose a guest OS type whose disk should be
attached. This could be done by having the `VmFactory` keep a ref to the
`ArtifactStore` that was used to initialize it and referring to that store when
adding a disk.

Complicating matters here is that today, the store allows lookup only by
arbitrary keys supplied in the artifact TOML, and there is no way to ensure that
the key a test will use is the one that's in the TOML, or that two tests that
want to refer to the same logical artifact will use the same key. To prevent
this sort of drift, tests should be able to say, e.g., "attach an Ubuntu 20.04
image, please," and the artifact store should decide which of its entries (if
any) is appropriate to yield for that guest OS kind.

== Support parameterized test cases

Parameterized testing is useful for testing multiple different machine
configurations and guest OS types. It would obviate the need for specifying
guest OS types on the command line--tests could just be parameterized with a
specific set of guest OS kinds/keys and instantiate VMs with the correct boot
disks based on their test parameters. (This requires the affordances for
selecting non-default guest OS artifacts described above.)

Existing parameterized-testing crates like `parameterized` change the signatures
of the functions they decorate. This doesn't work well with the way PHD uses the
`inventory` crate to collect tests for execution.

An alternative approach would be to amend the `#[phd_testcase]` attribute macro
to take a list of parameters that could then be injected into the context in
which the test body runs. For example, `#[phd_testcase(parameters = [(guest_os,
["alpine", "debian"], (cpus, [2, 4, 6, 8])])` could translate into something
along the lines of

```rust
// The fn name is macro-generated (replacing the `#fn_sig` insertion in the
// existing macro).
fn ncpus_test_guest_os_alpine_cpus_2(ctx: &TestContext) -> TestOutcome {
    // The macro inserts these.
    let __PHD_PARAM_guest_os = "alpine";
    let __PHD_PARAM_cpus = 2;

    match || -> phd_testcase::Result<()> {
        // Test body goes here, as in the current macro.
    }(){
    // etc.
    }
}
```

Then the macro crate can define a macro like `phd_param!(guest_os)` to get
inline insertion of `__PHD_PARAM_guest_os` into the test case.

== Support for skipped tests

Sometimes it's useful to run a test for a large set of parameters ("all Linux
guests") even though there are some exceptional parameters for which the test
can't run ("except $DISTRO_$VERSION, because it has a bug").

The runner and the `TestOutcome` enum support the notion of a "skipped" test for
cases like these. The idea is that if a test detects a condition like this, it
can return a special error code or (more likely) add a `phd_skip!()` macro that
marks the test outcome as skipped. This is distinct from failure--the test is
deliberately refusing to run because some preconditions to its success are not
satisfied, and it wants to signal that without actually producing a failing
result.

The main challenge here is figuring out how to work this into the existing
`#[phd_testcase]` macro. That macro wraps test function bodies in an
immediately-executed closure to allow those functions to use the `?` operator
for operations that might fail. Either this framework needs to be revisited, or
the hypothetical `phd_skip!()` needs to do something that short-circuits this
process. (Perhaps it can just `return phd_testcase::TestOutcome::Skipped` from
wherever the macro is invoked?)
